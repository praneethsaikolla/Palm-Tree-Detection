import os
import json
from PIL import Image
import torch
from torch.utils.data import Dataset

class PalmTreeDataset(Dataset):
    def __init__(self, coco_annotation_file, image_dir, transforms=None):
        self.image_dir = image_dir
        self.transforms = transforms

        # Load COCO annotations
        with open(coco_annotation_file, 'r') as f:
            self.coco_data = json.load(f)

        # Index images and annotations
        self.images = {img['id']: img for img in self.coco_data['images']}
        self.annotations = self.coco_data['annotations']

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        # Get image information
        image_info = self.images[idx + 1]  # COCO IDs start at 1
        image_path = os.path.join(self.image_dir, image_info['file_name'])

        # Load image
        image = Image.open(image_path).convert("RGB")

        # Get annotations for the image
        image_id = image_info['id']
        annotations = [anno for anno in self.annotations if anno['image_id'] == image_id]

        # Prepare target dictionary
        boxes = []
        labels = []
        for anno in annotations:
            bbox = anno['bbox']
            x_min, y_min, width, height = bbox
            x_max = x_min + width
            y_max = y_min + height
            boxes.append([x_min, y_min, x_max, y_max])
            labels.append(anno['category_id'])

        target = {
            'boxes': torch.tensor(boxes, dtype=torch.float32),
            'labels': torch.tensor(labels, dtype=torch.int64),
            'image_id': torch.tensor([image_id])
        }

        # Apply transforms
        if self.transforms:
            image = self.transforms(image)

        return image, target
from torch.utils.data import DataLoader
from torchvision.transforms import Compose, ToTensor

# Initialize Dataset and DataLoader
train_dataset = PalmTreeDataset(
    coco_annotation_file='/kaggle/input/palm-tree-annotations/annotations/train_annotations.json',
    image_dir='/kaggle/input/palm-tree-detection-dataset/Palm Tree Detection Dataset/Palm-Counting-349images/train/images',
    transforms=Compose([ToTensor()])
)

train_loader = DataLoader(
    train_dataset,
    batch_size=8,  # Adjust batch size as per your GPU memory
    shuffle=True,
    collate_fn=lambda x: tuple(zip(*x))  # Collate function for variable-sized targets
)
import torchvision
from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

def get_faster_rcnn_model(num_classes):
    # Load model with default COCO weights
    weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT
    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)
    
    # Update the classifier head for the number of classes
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
    
    return model

# Instantiate the model
model = get_faster_rcnn_model(num_classes=2)  # 2 classes: background + palm_tree
import torch

# Training setup
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)
optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)

# Training loop
num_epochs = 50  # Adjust as needed
for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0

    for images, targets in train_loader:
        images = [img.to(device) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())

        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

        epoch_loss += losses.item()

    print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}")

# Save the trained model
torch.save(model.state_dict(), '/kaggle/working/faster_rcnn_palm_tree.pth')
print("Model saved!")
# Import necessary libraries
import torch
from torchvision.transforms import ToTensor
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from torchvision.models.detection import fasterrcnn_resnet50_fpn

# Define prediction function
def predict(image_path, model, device):
    model.eval()
    image = Image.open(image_path).convert("RGB")
    image_tensor = ToTensor()(image).to(device)

    with torch.no_grad():
        prediction = model([image_tensor])
    return prediction[0]

# Define visualization function to display original and predictions side by side
def visualize_predictions_with_original(image_path, predictions, threshold=0.5):
    # Load the original image
    image = Image.open(image_path).convert("RGB")
    
    # Create subplots for original image and predictions
    fig, axes = plt.subplots(1, 2, figsize=(16, 8))
    
    # Display the original image
    axes[0].imshow(image)
    axes[0].set_title("Original Image")
    axes[0].axis('off')

    # Display the image with predictions
    axes[1].imshow(image)
    axes[1].set_title("Predictions")
    axes[1].axis('off')

    # Add bounding boxes and scores to the second subplot
    for box, score in zip(predictions['boxes'], predictions['scores']):
        if score < threshold:
            continue
        xmin, ymin, xmax, ymax = box.tolist()
        width, height = xmax - xmin, ymax - ymin

        # Add bounding box
        rect = patches.Rectangle(
            (xmin, ymin), width, height, linewidth=2, edgecolor='red', facecolor='none'
        )
        axes[1].add_patch(rect)
        
        # Add score label
        axes[1].text(xmin, ymin - 5, f"{score:.2f}", color='red', fontsize=10,
                     bbox=dict(facecolor='white', alpha=0.5))

    # Adjust layout and display
    plt.tight_layout()
    plt.show()


# Set up device
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

# Load the pre-trained Faster R-CNN model
model = fasterrcnn_resnet50_fpn(pretrained=False, num_classes=2)  # Assuming 2 classes: background + palm_tree
model.load_state_dict(torch.load('/kaggle/working/faster_rcnn_palm_tree.pth', map_location=device))
model.to(device)

# Test the model with an example image
test_image_path = '/kaggle/input/palm-tree-detection-test-image/Palm Tree Detection Dataset Image.jpg'  # Path to test image
predictions = predict(test_image_path, model, device)

# Test the model with an example image and visualize both original and predictions
visualize_predictions_with_original(test_image_path, predictions)
# Create test dataset and DataLoader
test_dataset = PalmTreeDataset(
    coco_annotation_file='/kaggle/input/palm-tree-annotations/annotations/test_annotations.json',
    image_dir='/kaggle/input/palm-tree-detection-dataset/Palm Tree Detection Dataset/Palm-Counting-349images/test/images',
    transforms=Compose([ToTensor()])  # Add augmentations if required
)

test_loader = DataLoader(
    test_dataset,
    batch_size=8,  # Adjust batch size
    shuffle=False,  # No need to shuffle test data
    collate_fn=lambda x: tuple(zip(*x))  # Handle variable-sized targets
)
from torchvision.ops import box_iou

def evaluate_model_on_test_set(model, test_loader, device, iou_threshold=0.5):
    model.eval()
    total_loss = 0
    correct_predictions = 0
    total_predictions = 0

    all_scores = []
    all_iou = []

    with torch.no_grad():
        for images, targets in test_loader:
            images = [img.to(device) for img in images]
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            # Forward pass
            outputs = model(images)

            for idx, output in enumerate(outputs):
                # Calculate IoU between predicted and ground-truth boxes
                gt_boxes = targets[idx]['boxes'].cpu()
                pred_boxes = output['boxes'].cpu()
                pred_scores = output['scores'].cpu()

                if len(pred_boxes) == 0 or len(gt_boxes) == 0:
                    continue

                ious = box_iou(gt_boxes, pred_boxes)
                max_iou, max_idx = ious.max(dim=1)

                # Filter predictions by IoU threshold
                valid_preds = max_iou >= iou_threshold
                correct_predictions += valid_preds.sum().item()
                total_predictions += len(gt_boxes)

                # Store scores and IoU for later analysis
                all_scores.extend(pred_scores.tolist())
                all_iou.extend(max_iou.tolist())

    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
    print(f"Accuracy: {accuracy:.2f}")
    return accuracy, all_scores, all_iou
accuracy, all_scores, all_iou = evaluate_model_on_test_set(model, test_loader, device)
print(f"Test Accuracy: {accuracy:.4f}")
from torchvision.ops import box_iou
import numpy as np

def evaluate_model_with_metrics(model, test_loader, device, iou_threshold=0.5, score_threshold=0.5):
    model.eval()

    # Initialize counters
    true_positives = 0
    false_positives = 0
    false_negatives = 0
    all_ap = []

    with torch.no_grad():
        for images, targets in test_loader:
            images = [img.to(device) for img in images]
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            # Forward pass
            outputs = model(images)

            for idx, output in enumerate(outputs):
                gt_boxes = targets[idx]['boxes'].cpu()
                pred_boxes = output['boxes'].cpu()
                pred_scores = output['scores'].cpu()

                if len(pred_boxes) == 0:
                    false_negatives += len(gt_boxes)
                    continue

                if len(gt_boxes) == 0:
                    false_positives += len(pred_boxes)
                    continue

                # Filter predictions based on score threshold
                valid_preds = pred_scores >= score_threshold
                pred_boxes = pred_boxes[valid_preds]
                pred_scores = pred_scores[valid_preds]

                # Calculate IoU
                ious = box_iou(gt_boxes, pred_boxes)
                max_iou, max_idx = ious.max(dim=1)

                # Count True Positives, False Positives, and False Negatives
                tp = (max_iou >= iou_threshold).sum().item()
                fp = len(pred_boxes) - tp
                fn = len(gt_boxes) - tp

                true_positives += tp
                false_positives += fp
                false_negatives += fn

                # Compute average precision for this image (optional for mAP)
                precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                recall = tp / (tp + fn) if (tp + fn) > 0 else 0
                all_ap.append(precision)

    # Calculate Precision, Recall, F1-Score
    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    mAP = np.mean(all_ap) if len(all_ap) > 0 else 0

    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1-Score: {f1_score:.2f}")
    print(f"mAP: {mAP:.2f}")

    return {
        "precision": precision,
        "recall": recall,
        "f1_score": f1_score,
        "mAP": mAP
    }

# Evaluate model on the test dataset
metrics = evaluate_model_with_metrics(model, test_loader, device)
from sklearn.metrics import confusion_matrix
import seaborn as sns
import torch
import numpy as np
import matplotlib.pyplot as plt
from torchvision.ops import box_iou

def calculate_confusion_matrix(model, test_loader, device, iou_threshold=0.5, score_threshold=0.5):
    model.eval()
    
    y_true = []
    y_pred = []
    
    with torch.no_grad():
        for images, targets in test_loader:
            images = [img.to(device) for img in images]
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
            
            # Forward pass
            outputs = model(images)

            for idx, output in enumerate(outputs):
                # Get ground truth and predicted boxes as tensors
                gt_boxes = torch.tensor(targets[idx]['boxes'].cpu().numpy(), dtype=torch.float32)
                pred_boxes = torch.tensor(output['boxes'].cpu().numpy(), dtype=torch.float32)
                pred_scores = output['scores'].cpu().numpy()

                # Filter predictions based on score threshold
                valid_preds = pred_scores >= score_threshold
                pred_boxes = pred_boxes[valid_preds]

                # Compute IoU between predicted and ground truth boxes
                if len(gt_boxes) > 0 and len(pred_boxes) > 0:
                    ious = box_iou(gt_boxes, pred_boxes)

                    # For each ground truth, check if it is matched by a prediction (True Positive or False Negative)
                    for gt_box in gt_boxes:
                        # Get max IoU for each ground truth box
                        max_iou, _ = ious.max(dim=1)  # Max IoU with all predicted boxes
                        
                        # Iterate over all IoU values for the current gt_box and count true positives based on threshold
                        match_found = False
                        for iou in max_iou:
                            if iou >= iou_threshold:
                                y_true.append(1)  # True positive
                                y_pred.append(1)
                                match_found = True
                                break
                        if not match_found:
                            y_true.append(1)  # False negative
                            y_pred.append(0)
                
                    # For each predicted box, check if it is a False Positive (no matching ground truth)
                    for pred_box in pred_boxes:
                        # Get max IoU for each predicted box
                        max_iou, _ = ious.max(dim=0)  # Max IoU with all ground truth boxes
                        
                        # Compare each max_iou element-wise against the threshold for False Positive detection
                        for iou in max_iou:
                            if iou < iou_threshold:
                                y_true.append(0)  # False positive
                                y_pred.append(1)

    cm = confusion_matrix(y_true, y_pred)
    return cm

# Calculate confusion matrix
cm = calculate_confusion_matrix(model, test_loader, device)

# Plot confusion matrix
plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['No Palm Tree', 'Palm Tree'], yticklabels=['No Palm Tree', 'Palm Tree'])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()
import numpy as np
from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt
from torchvision.ops import box_iou

def calculate_precision_recall(model, test_loader, device, iou_threshold=0.5, score_threshold=0.5):
    model.eval()
    
    # List to store true labels (ground truth) and predicted scores (for calculating precision-recall)
    y_true = []
    y_scores = []
    
    with torch.no_grad():
        for images, targets in test_loader:
            images = [img.to(device) for img in images]
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
            
            # Forward pass
            outputs = model(images)

            for idx, output in enumerate(outputs):
                # Get ground truth boxes and predicted boxes
                gt_boxes = torch.tensor(targets[idx]['boxes'].cpu().numpy(), dtype=torch.float32)
                pred_boxes = torch.tensor(output['boxes'].cpu().numpy(), dtype=torch.float32)
                pred_scores = output['scores'].cpu().numpy()
                
                # Filter predictions based on score threshold
                valid_preds = pred_scores >= score_threshold
                pred_boxes = pred_boxes[valid_preds]
                pred_scores = pred_scores[valid_preds]

                # If no predictions, skip
                if len(pred_boxes) == 0:
                    continue

                # Compute IoU between predicted and ground truth boxes
                ious = box_iou(gt_boxes, pred_boxes)

                # For each ground truth, check if it is matched by a prediction
                for gt_box in gt_boxes:
                    max_iou, _ = ious.max(dim=1)  # Max IoU for each ground truth box
                    
                    # True positive if max IoU is above the threshold
                    match_found = False
                    for iou in max_iou:
                        if iou >= iou_threshold:
                            y_true.append(1)  # True positive
                            y_scores.append(np.max(pred_scores))  # Prediction score
                            match_found = True
                            break
                    if not match_found:
                        y_true.append(1)  # False negative
                        y_scores.append(0)  # No score for false negative

                # For each predicted box, check if it is a False Positive
                for pred_box in pred_boxes:
                    max_iou, _ = ious.max(dim=0)  # Max IoU for each predicted box
                    
                    # False positive if max IoU is below the threshold
                    for iou in max_iou:
                        if iou < iou_threshold:
                            y_true.append(0)  # False positive
                            y_scores.append(np.max(pred_scores))  # Prediction score

    # Calculate Precision-Recall curve
    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)

    return precision, recall

# Calculate precision and recall
precision, recall = calculate_precision_recall(model, test_loader, device)

# Plot Precision-Recall curve
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, marker='.', color='blue', label='Precision-Recall curve')
plt.title("Precision-Recall Curve")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.legend(loc='best')
plt.grid(True)
plt.show()
import numpy as np
import matplotlib.pyplot as plt

# Sort scores for a smooth line plot
sorted_scores = np.sort(all_scores)
sorted_ious = np.sort(all_iou)

# Generate frequency for scores (cumulative distribution function for better clarity)
score_frequencies = np.arange(len(sorted_scores)) / float(len(sorted_scores))
iou_frequencies = np.arange(len(sorted_ious)) / float(len(sorted_ious))

# Plot line graph for prediction scores
plt.figure(figsize=(10, 6))
plt.plot(sorted_scores, score_frequencies, label="Prediction Scores", color='blue')
plt.title("Prediction Score Distribution (Line Graph)")
plt.xlabel("Score")
plt.ylabel("Cumulative Frequency")
plt.grid(True)
plt.legend()
plt.show()

# Plot line graph for IoU
plt.figure(figsize=(10, 6))
plt.plot(sorted_ious, iou_frequencies, label="IoU", color='green')
plt.title("IoU Distribution (Line Graph)")
plt.xlabel("IoU")
plt.ylabel("Cumulative Frequency")
plt.grid(True)
plt.legend()
plt.show()
